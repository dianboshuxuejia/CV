{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ceb9f3",
   "metadata": {},
   "source": [
    "## Demo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef7696",
   "metadata": {},
   "source": [
    "### Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14236d",
   "metadata": {},
   "source": [
    "First look at the project directory:\n",
    "\n",
    "*VOCdevkit:* holds the training data\n",
    "\n",
    "*weights:* the weights file\n",
    "\n",
    "*Config.py:* some default configuration\n",
    "\n",
    "*Test.py:* test the recognition of a single photo\n",
    "\n",
    "*Train.py:* the py file for training\n",
    "\n",
    "*augmentation.py:* py file for data augmentation, the main function is to expand the training data\n",
    "\n",
    "*detection.py:* partial filtering of the data from the recognition results, which is transferred to the Test.py file for use in its calls\n",
    "\n",
    "*l2norm.py:* performs l2 regularisation\n",
    "\n",
    "*loss_function.py:* compute the loss function\n",
    "\n",
    "*ssd_net_vgg.py:* implementation of the ssd model\n",
    "\n",
    "*utils.py:* tool classes\n",
    "\n",
    "*voc0712.py:* rewrite the dataset class to extract and regularize the data from the voc\n",
    "\n",
    "*visdom_op.py:* define visdom class to visualize the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6043b",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87ffea",
   "metadata": {},
   "source": [
    "We build model in file *ssd_net_vgg.py* accordding to the logic of the paper which we implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32dc540",
   "metadata": {},
   "source": [
    "### Calculate Default Box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709711ec",
   "metadata": {},
   "source": [
    "The code is in *utils.py* file which is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e737cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_prior_box():\n",
    "    mean_layer = []\n",
    "    for k,f in enumerate(Config.feature_map):\n",
    "        mean = []\n",
    "        for i,j in product(range(f),repeat=2):\n",
    "            f_k = Config.image_size/Config.steps[k]\n",
    "            cx = (j+0.5)/f_k\n",
    "            cy = (i+0.5)/f_k\n",
    "\n",
    "            s_k = Config.sk[k]/Config.image_size\n",
    "            mean += [cx,cy,s_k,s_k]\n",
    "\n",
    "            s_k_prime = sqrt(s_k * Config.sk[k+1]/Config.image_size)\n",
    "            mean += [cx,cy,s_k_prime,s_k_prime]\n",
    "            for ar in Config.aspect_ratios[k]:\n",
    "                mean += [cx, cy, s_k * sqrt(ar), s_k/sqrt(ar)]\n",
    "                mean += [cx, cy, s_k / sqrt(ar), s_k * sqrt(ar)]\n",
    "        if Config.use_cuda:\n",
    "            mean = torch.Tensor(mean).cuda().view(Config.feature_map[k], Config.feature_map[k], -1).contiguous()\n",
    "        else:\n",
    "            mean = torch.Tensor(mean).view( Config.feature_map[k],Config.feature_map[k],-1).contiguous()\n",
    "        mean.clamp_(max=1, min=0)\n",
    "        mean_layer.append(mean)\n",
    "\n",
    "    return mean_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd0181",
   "metadata": {},
   "source": [
    "The function then generates boxes, corresponding to the number in the paper, and the final output is a list of 6, each list corresponding to the number of default boxes output by a feature layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea69d71",
   "metadata": {},
   "source": [
    "### Calculate Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ebfc8b",
   "metadata": {},
   "source": [
    "Loss function is in *loss_function.py* of which core part is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4053da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFun(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossFun,self).__init__()\n",
    "    def forward(self, prediction,targets,priors_boxes):\n",
    "        loc_data , conf_data = prediction\n",
    "        loc_data = torch.cat([o.view(o.size(0),-1,4) for o in loc_data] ,1)\n",
    "        conf_data = torch.cat([o.view(o.size(0),-1,Config.class_num) for o in conf_data],1)\n",
    "        priors_boxes = torch.cat([o.view(-1,4) for o in priors_boxes],0)\n",
    "        if Config.use_cuda:\n",
    "            loc_data = loc_data.cuda()\n",
    "            conf_data = conf_data.cuda()\n",
    "            priors_boxes = priors_boxes.cuda()\n",
    "        # batch_size\n",
    "        batch_num = loc_data.size(0)\n",
    "        # default_box number\n",
    "        box_num = loc_data.size(1)\n",
    "        # store targets according to each prior_box date after transformation\n",
    "        target_loc = torch.Tensor(batch_num,box_num,4)\n",
    "        target_loc.requires_grad_(requires_grad=False)\n",
    "        # store each type of prediction of default_box\n",
    "        target_conf = torch.LongTensor(batch_num,box_num)\n",
    "        target_conf.requires_grad_(requires_grad=False)\n",
    "        if Config.use_cuda:\n",
    "            target_loc = target_loc.cuda()\n",
    "            target_conf = target_conf.cuda()\n",
    "        # Since there may be multiple graphs in a batch, each loop computes the loc and conf of one box in the graph, i.e. 8732 boxes, which are stored in target_loc and target_conf\n",
    "        for batch_id in range(batch_num):\n",
    "            target_truths = targets[batch_id][:,:-1].data\n",
    "            target_labels = targets[batch_id][:,-1].data\n",
    "            if Config.use_cuda:\n",
    "                target_truths = target_truths.cuda()\n",
    "                target_labels = target_labels.cuda()\n",
    "            # Calculate the box function, i.e. the formula for the loc loss function in Eq.\n",
    "            utils.match(0.5,target_truths,priors_boxes,target_labels,target_loc,target_conf,batch_id)\n",
    "        pos = target_conf > 0\n",
    "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
    "        # Equivalent to the operation of multiplying xij by the L1 loss function in the paper\n",
    "        pre_loc_xij = loc_data[pos_idx].view(-1,4)\n",
    "        tar_loc_xij = target_loc[pos_idx].view(-1,4)\n",
    "        # Smooth_li loss function by taking the calculated loc and prediction\n",
    "        loss_loc = F.smooth_l1_loss(pre_loc_xij,tar_loc_xij,size_average=False)\n",
    "\n",
    "        batch_conf = conf_data.view(-1,Config.class_num)\n",
    "\n",
    "        # Referring to the conf calculation in the paper, find the ci\n",
    "        loss_c = utils.log_sum_exp(batch_conf) - batch_conf.gather(1, target_conf.view(-1, 1))\n",
    "\n",
    "        loss_c = loss_c.view(batch_num, -1)\n",
    "        # Set positive sample to 0\n",
    "        loss_c[pos] = 0\n",
    "\n",
    "        # Sort the remaining negative samples and select the target number of negative samples\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "\n",
    "        num_pos = pos.long().sum(1, keepdim=True)\n",
    "        num_neg = torch.clamp(3*num_pos, max=pos.size(1)-1)\n",
    "\n",
    "        # Extraction of positive and negative samples\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "\n",
    "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, Config.class_num)\n",
    "        targets_weighted = target_conf[(pos+neg).gt(0)]\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n",
    "\n",
    "        N = num_pos.data.sum().double()\n",
    "        loss_l = loss_loc.double()\n",
    "        loss_c = loss_c.double()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "        return loss_l, loss_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93520644",
   "metadata": {},
   "source": [
    "### Match function\n",
    "\n",
    "We think this function is a difficult part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6dd3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(threshold, truths, priors, labels, loc_t, conf_t, idx):\n",
    "    \"\"\"\n",
    "    Calculate the jaccard ratio between default box and actual location, calculate the maximum jaccard ratio of each box for each kind and the maximum jaccard ratio of each kind of box\n",
    "    Args.\n",
    "        threshold: (float) The threshold of the jaccard ratio.\n",
    "        truths: (tensor) The actual position.\n",
    "        priors: (tensor) default box\n",
    "        labels: (tensor) The actual number of categories an image contains.\n",
    "        loc_t: (tensor) The maximum jaccard ratio that needs to be stored for each box in each category.\n",
    "        conf_t: (tensor) The category that stores the maximum jaccard ratio for each box.\n",
    "        idx: (int) The current batch\n",
    "    \"\"\"\n",
    "    # jaccard\n",
    "    overlaps = jaccard(\n",
    "        truths,\n",
    "        # Convert priors to x_min,y_min,x_max and y_max\n",
    "        point_form(priors)\n",
    "    )\n",
    "    # [1,num_objects] best prior for each ground truth\n",
    "    # The actual category contained corresponds to the box with the largest jaccarb in the box and the corresponding index value, i.e. the best box for each category\n",
    "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n",
    "    # [1,num_priors] best ground truth for each prior\n",
    "    # For each box, the category with the largest jaccard ratio in the actual category, i.e. the optimal category for each box\n",
    "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n",
    "    best_truth_idx.squeeze_(0)\n",
    "    best_truth_overlap.squeeze_(0)\n",
    "    best_prior_idx.squeeze_(1)\n",
    "    best_prior_overlap.squeeze_(1)\n",
    "    # Set the maximum box in each category to 2 to ensure that it does not affect later operations\n",
    "    best_truth_overlap.index_fill_(0, best_prior_idx, 2)\n",
    "\n",
    "    # Calculate the optimal class for each box, and the optimal loc for each class\n",
    "    for j in range(best_prior_idx.size(0)):\n",
    "        best_truth_idx[best_prior_idx[j]] = j\n",
    "    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n",
    "    conf = labels[best_truth_idx] + 1         # Shape: [num_priors]\n",
    "    conf[best_truth_overlap < threshold] = 0  # label as background\n",
    "    # Implement the conversion of loc, the specific conversion formula refer to the formula of the loss function of loc in the paper\n",
    "    loc = encode(matches, priors,(0.1,0.2))\n",
    "    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n",
    "    conf_t[idx] = conf  # [num_priors] top class label for each prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b1b66",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Core part of trainning like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e84a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    dataset = voc0712.VOCDetection(root=Config.dataset_root,\n",
    "                           transform=augmentations.SSDAugmentation(Config.image_size,\n",
    "                                                     Config.MEANS))\n",
    "    data_loader = data.DataLoader(dataset, Config.batch_size,\n",
    "                                  num_workers=Config.data_load_number_worker,\n",
    "                                  shuffle=True, collate_fn=detection_collate,\n",
    "                                  pin_memory=True,generator=torch.Generator(device='cuda'))\n",
    "\n",
    "    net = ssd_net_vgg.SSD()\n",
    "    vgg_weights = torch.load('./weights/vgg16_reducedfc.pth')\n",
    "\n",
    "    #visualization setting\n",
    "    vis = setup_visdom()\n",
    "    vis_step = 3\n",
    "\n",
    "    net.apply(weights_init)\n",
    "    net.vgg.load_state_dict(vgg_weights)\n",
    "    # net.apply(weights_init)\n",
    "    if Config.use_cuda:\n",
    "        net = torch.nn.DataParallel(net)\n",
    "        net = net.cuda()\n",
    "    net.train()\n",
    "    loss_fun = loss_function.LossFun()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=Config.lr, momentum=Config.momentum,\n",
    "                          weight_decay=Config.weight_decacy)\n",
    "    iter = 0\n",
    "    step_index = 0\n",
    "    before_epoch = -1\n",
    "    for epoch in range(1000):\n",
    "        for step,(img,target) in enumerate(data_loader):\n",
    "            if Config.use_cuda:\n",
    "                img = img.cuda()\n",
    "                target = [ann.cuda() for ann in target]\n",
    "            img = torch.Tensor(img)\n",
    "            loc_pre,conf_pre = net(img)\n",
    "            priors = utils.default_prior_box()\n",
    "            optimizer.zero_grad()\n",
    "            loss_l,loss_c = loss_fun((loc_pre,conf_pre),target,priors)\n",
    "            loss = loss_l + loss_c\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iter % 3 == 0 or before_epoch!=epoch:\n",
    "                print('epoch : ',epoch,' iter : ',iter,' step : ',step,' loss : ',loss.data,'loss_l:',loss_l.data,'loss_c:',loss_c.data)\n",
    "                before_epoch = epoch\n",
    "            iter+=1\n",
    "            \n",
    "            if vis and iter % vis_step == 0:\n",
    "                visdom_line(vis, y=[loss], x=iter, win_name='loss')\n",
    "                visdom_line(vis, y=[loss_c], x=iter, win_name='loss_c')\n",
    "                visdom_line(vis, y=[loss_l], x=iter, win_name='loss_l')\n",
    "            if iter in Config.lr_steps:\n",
    "                step_index+=1\n",
    "                adjust_learning_rate(optimizer,Config.gamma,step_index)\n",
    "            if iter % 10000 == 0 and iter!=0:\n",
    "                torch.save(net.state_dict(), 'weights/ssd300_VOC_' +\n",
    "                           repr(iter) + '.pth')\n",
    "        if iter >= Config.max_iter:\n",
    "            break\n",
    "    torch.save(net.state_dict(), 'weights/ssd_voc_120000.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9bbaf",
   "metadata": {},
   "source": [
    "### Test for one image\n",
    "\n",
    "**Run this part of code, you will see the result of dectection on an image.Be care that the jupyter notebook file should be put in the origin path.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca1d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from detection import *\n",
    "from ssd_net_vgg import *\n",
    "from voc0712 import *\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import utils\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "colors_tableau = [(255, 255, 255), (31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),\n",
    "                 (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),\n",
    "                 (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),\n",
    "                 (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),\n",
    "                 (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229),(158, 218, 229),(158, 218, 229)]\n",
    "\n",
    "net = SSD()    # initialize SSD\n",
    "net = torch.nn.DataParallel(net)\n",
    "net.train(mode=False)\n",
    "net.load_state_dict(torch.load('./weights/ssd300_VOC_120000.pth',map_location=lambda storage, loc: storage))\n",
    "img_id = 60\n",
    "image = cv2.imread('./test1.jpg', cv2.IMREAD_COLOR)\n",
    "x = cv2.resize(image, (300, 300)).astype(np.float32)\n",
    "x -= (104.0, 117.0, 123.0)\n",
    "x = x.astype(np.float32)\n",
    "x = x[:, :, ::-1].copy()\n",
    "# plt.imshow(x)\n",
    "x = torch.from_numpy(x).permute(2, 0, 1)\n",
    "xx = Variable(x.unsqueeze(0))     # wrap tensor in Variable\n",
    "if torch.cuda.is_available():\n",
    "    xx = xx.cuda()\n",
    "y = net(xx)\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "detect = Detect(config.class_num, 0, 200, 0.01, 0.45)\n",
    "priors = utils.default_prior_box()\n",
    "\n",
    "loc,conf = y\n",
    "loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
    "\n",
    "detections = detect.forward(loc.view(loc.size(0), -1, 4),softmax(conf.view(conf.size(0), -1,config.class_num)), torch.cat([o.view(-1, 4) for o in priors], 0)).data\n",
    "\n",
    "labels = VOC_CLASSES\n",
    "top_k=10\n",
    "\n",
    "# plt.imshow(rgb_image)  # plot the image for matplotlib\n",
    "\n",
    "# scale each detection back up to the image\n",
    "scale = torch.Tensor(image.shape[1::-1]).repeat(2)\n",
    "for i in range(detections.size(1)):\n",
    "    j = 0\n",
    "    while detections[0,i,j,0] >= 0.4:\n",
    "        score = detections[0,i,j,0]\n",
    "        label_name = labels[i-1]\n",
    "        display_txt = '%s: %.2f'%(label_name, score)\n",
    "        pt = (detections[0,i,j,1:]*scale).cpu().numpy()\n",
    "        coords = (pt[0], pt[1]), pt[2]-pt[0]+1, pt[3]-pt[1]+1\n",
    "        color = colors_tableau[i]\n",
    "        cv2.rectangle(image,(int(pt[0]),int(pt[1])), (int(pt[2]),int(pt[3])), color, 2)\n",
    "        cv2.putText(image, display_txt, (int(pt[0]), int(pt[1]) + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, 8)\n",
    "        j+=1\n",
    "cv2.imshow('test',image)\n",
    "cv2.waitKey(100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0fc98",
   "metadata": {},
   "source": [
    "The input image is ![avatar](test1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a96f2d",
   "metadata": {},
   "source": [
    "The output image is ![avatar](result.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd79305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
